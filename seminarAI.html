<h3>Seminarium metody AI</h3>
<p>Termin zajęć:  poniedziałek 12.00 - 13.30, sala: 1146</p>
<ul>
	<a href="metodyAI-zasady.pdf">Zasady seminarium</a>
</ul>
<h4>Terminarz:</h4>
<ul>
	<li class="item">14.10.2024: Tomasz Szczepanik: <a href="https://arxiv.org/pdf/2106.15147v2">SCARF: SELF-SUPERVISED CONTRASTIVE LEARNING USING RANDOM FEATURE CORRUPTION</a></li>
	<li class="item">14.10.2024: Michał Latra: <a href="https://proceedings.mlr.press/v235/kim24d.html">CARTE: Pretraining and Transfer for Tabular Learning</a></li>
	<li class="item">21.10.2024: Mikołaj Janusz</li>
	<li class="item">28.10.2024: Kornel Howil</li>
	<li class="item">4.11.2024: Filip Soszyński, Adam Egner</li>
	<li class="item">18.11.2024: Maciej Rut, Jakub Steczkiewicz</li>
	<li class="item">25.11.2024: Piotr Pena, Patryk Marszałek</li>
	<li class="item">2.12.2024: Peweł Prochot, Tomasz Wojnar</li>
	<li class="item">9.12.2024: Michał Bednarczyk, Illia Dovhalenko</li>
	<li class="item">16.12.2024: Tomasz Dądela</li>
	<li class="item">13.10.2024: Piotr Kubaty</li>
	<li class="item">20.10.2024: </li>
	<li class="item">27.10.2024: </li>
</ul>

<h4>Propozycje tematów:</h4>
<ul>
	<li class="item"><a href="https://openreview.net/pdf?id=_xlsjehDvlY">STUNT: FEW-SHOT TABULAR LEARNING WITH SELF-GENERATED TASKS FROM UNLABELED TABLES</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2305.06090">XTab: Cross-table Pretraining for Tabular Transformers</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2108.09084">Fastformer: Additive Attention Can Be All You Need</a></li>
	<li class="item"><a href="https://openreview.net/pdf?id=A1yGs_SWiIi">TransTab: Learning Transferable Tabular Transformers Across Tables</a></li>
	<li class="item"><a href="https://arxiv.org/abs/2305.02997">When Do Neural Nets Outperform Boosted Trees on Tabular Data?</a></li>
	<li class="item"><a href="https://openreview.net/pdf?id=GGylthmehy">High dimensional, tabular deep learning with an auxiliary knowledge graph</a></li>
	<li class="item"><a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/kim24d/kim24d.pdf">CARTE: Pretraining and Transfer for Tabular Learning</a></li>
	<li class="item"><a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/ren24b/ren24b.pdf">TabLog: Test-Time Adaptation for Tabular Data Using Logic Rules</a></li>
	<li class="item"><a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/svirsky24a/svirsky24a.pdf">Interpretable Deep Clustering for Tabular Data</a></li>
	<li class="item"><a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/van-breugel24a/van-breugel24a.pdf">Position: Why Tabular Foundation Models Should Be a Research Priority</a></li>
	<li class="item"><a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/vero24a/vero24a.pdf">CuTS: Customizable Tabular Synthetic Data Generation</a></li>
	<li class="item"><a href="ttps://proceedings.neurips.cc/paper_files/paper/2023/file/0731f0e65559059eb9cd9d6f44ce2dd8-Paper-Conference.pdf">Forecastpfn: Synthetically-trained zero-shot forecasting</a></li>
	<li class="item"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28988/29878">HyperFast: Instant Classification for Tabular Data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2402.06971">In-Context Data Distillation with TabPFN</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2405.19221">Domain adaptation in small-scale and heterogeneous biological datasets</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2407.04491">Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2308.00177">Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity</a></li>
	<li class="item"><a href="https://openreview.net/pdf?id=HtdZSf1ObU">Benchmarking Tabular Representation Models in Transfer Learning Settings</a></li>
	<li class="item"><a href="https://www.sciencedirect.com/science/article/pii/S0925231224007380?casa_token=mxju_nxIKCEAAAAA:Eg5pBLBMjcMgbfbB3wCz1IAatcvhGRinCeEYnsKz0kEfZvB1j9VsU4vrhx31usb8xbjZfZvG">Investigating latent representations and generalization in deep neural networks for tabular data</a></li>
	<li class="item"><a href="https://proceedings.mlr.press/v202/kotelnikov23a/kotelnikov23a.pdf">Tabddpm: Modelling tabular data with diffusion models</a></li>
	<li class="item"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9e9f0ffc3d836836ca96cbf8fe14b105-Paper-Conference.pdf">On embeddings for numerical features in tabular deep learning</a></li>
	<li class="item"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a76a757ed479a1e6a5f8134bea492f83-Paper-Datasets_and_Benchmarks.pdf">Benchmarking distribution shift in tabular data with tableshift</a></li>
	<li class="item"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/66178beae8f12fcd48699de95acc1152-Paper-Conference.pdf">HYTREL: Hypergraph-enhanced tabular data representation learning</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2207.03208">Revisiting pretraining objectives for tabular deep learning</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2206.15306">Transfer learning with deep tabular models</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2209.08060">Ptab: Using the pre-trained language model for modeling tabular data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2307.04308">CT-BERT: learning better tabular representations through cross-table pre-training</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2403.01841">Making pre-trained language models great on tabular prediction</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2310.07338">Towards foundation models for learning on tabular data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2407.05364">PTaRL: Prototype-based tabular representation learning via space calibration</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2206.00664">Hopular: Modern hopfield networks for tabular data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2310.15149">Unlocking the transferability of tokens in deep models for tabular data</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2310.09656">Mixed-type tabular data synthesis with score-based diffusion in latent space</a></li>
	<li class="item"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/90debc7cedb5cac83145fc8d18378dc5-Paper-Conference.pdf">TabMT: Generating tabular data with masked transformers</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2307.00467">Missdiff: Training diffusion models on tabular data with missing values</a></li>
	<li class="item"><a href="https://openreview.net/pdf?id=r77CeOBO0L">Semi-supervised Tabular Classification via In-context Learning of Large Language Models</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2311.10051">Tabular few-shot generalization across heterogeneous feature spaces</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2404.09491">Large Language Models 
	Can Automatically Engineer Features for Few-Shot Tabular Learning</a></li>
	<li class="item"><a href="https://dl.acm.org/doi/pdf/10.1145/3637528.3671975">From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2408.11063">Tabular Transfer Learning via Prompting LLMs</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2406.08527">Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2405.13396">Why In-Context Learning Transformers are Tabular Data Classifiers</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2404.17489">Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2310.09278">Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2311.07343">Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2406.09837">TabularFM: An Open Framework For Tabular Foundational Models</a></li>
	<li class="item"><a href="https://arxiv.org/pdf/2307.02491">TablEye: Seeing small Tables through the Lens of Images</a></li>
	<li class="item"><a href="https://arxiv.org/abs/2207.01848">TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</a></li>
</ul>

